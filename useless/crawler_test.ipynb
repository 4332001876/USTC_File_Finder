{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin\n",
    "from config import Config\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# import pandas as pd\n",
    "import time\n",
    "import json\n",
    "\n",
    "\n",
    "class Crawler:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def fetch_data(self, url, encoding):\n",
    "        \"\"\"Fetch data from url. Return the content of the response.\"\"\"\n",
    "        time.sleep(Config.SLEEP_TIME)  # sleep for n second to avoid being blocked\n",
    "        headers = {\"User-Agent\": Config.USER_AGENT, 'Cookie': Config.COOKIE}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if encoding:\n",
    "            response.encoding = encoding\n",
    "        return response.text\n",
    "    \n",
    "    def fetch_file(self, url, html_locator, encoding):\n",
    "        \"\"\"Fetch file from url. Return a list of bs4.element.Tag.\"\"\"\n",
    "        bs = BeautifulSoup(self.fetch_data(url, encoding), 'html.parser')\n",
    "\n",
    "        for operation in html_locator:\n",
    "            method = operation['method']\n",
    "            args = operation['args']\n",
    "            kwargs = operation.get('kwargs', None)\n",
    "\n",
    "            if method == 'find':\n",
    "                if kwargs:\n",
    "                    bs = bs.find(args, **kwargs)\n",
    "                else:\n",
    "                    bs = bs.find(args)\n",
    "\n",
    "            elif method == 'find_all':\n",
    "                if kwargs:\n",
    "                    bs = bs.find_all(args, **kwargs)\n",
    "                else:\n",
    "                    bs = bs.find_all(args)\n",
    "\n",
    "        return bs\n",
    "    \n",
    "    def get_info(self, bs_result, url, method, args, kargs, args2):\n",
    "\n",
    "        if method == 'find' and kargs == None:\n",
    "            if args2 == 'text':\n",
    "                return bs_result.find(args).text.strip()\n",
    "            elif args2 == 'href':\n",
    "                return urljoin(url, bs_result.find(args)[args2])\n",
    "            else:\n",
    "                return bs_result.find(args)[args2]\n",
    "\n",
    "        elif method == 'find' and kargs != None:\n",
    "            if args2 == 'text':\n",
    "                return bs_result.find(args, **kargs).text.strip()\n",
    "            elif args2 == 'href':\n",
    "                return urljoin(url, bs_result.find(args, **kargs)[args2])\n",
    "            else:\n",
    "                return bs_result.find(args, **kargs)[args2]\n",
    "\n",
    "        elif method == None:\n",
    "            return None\n",
    "\n",
    "    def fetch_file_list(self, url, encoding, html_locator, info_locator , source_name, file_type = None, file_type_2 = None):\n",
    "        \"\"\"Fetch file list from url. Return a list of dict.\"\"\"\n",
    "        bs_result = self.fetch_file(url, html_locator, encoding)\n",
    "        \n",
    "        result = []\n",
    "\n",
    "        for i in range(len(bs_result)):\n",
    "            result.append(dict())\n",
    "            \n",
    "            for operation in info_locator:\n",
    "                info = operation['info']\n",
    "                method = operation['method']\n",
    "                args = operation['args']\n",
    "                args2 = operation['args2']\n",
    "                kargs = None\n",
    "\n",
    "                if type(args) == list:\n",
    "                    kargs = args[1]\n",
    "                    args = args[0]\n",
    "\n",
    "                result[i][info] = self.get_info(bs_result[i], url, method, args, kargs, args2)\n",
    "\n",
    "            \n",
    "            if result[i]['time'] != None and result[i]['time'].count('\\n') == 1:\n",
    "                parts = result[i]['time'].split('\\n')\n",
    "                result[i]['time'] = f'{parts[1]}-{parts[0]}'\n",
    "\n",
    "            result[i]['source'] = source_name\n",
    "            result[i]['file_type'] = file_type\n",
    "            result[i]['file_type_2'] = file_type_2\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def generate_file_list(self):\n",
    "        file_list = []\n",
    "        with open('data.json', 'r') as json_file:\n",
    "            data_list = json.load(json_file)\n",
    "\n",
    "        for data in data_list:\n",
    "            url = data['url']\n",
    "            encoding = data['encoding']\n",
    "            html_locator = data['html_locator']\n",
    "            info_locator = data['info_locator']\n",
    "            source_name = data['title']\n",
    "            file_type = data['dtype']\n",
    "            file_type_2 = data['dtype2']\n",
    "            flip = data['flip']\n",
    "\n",
    "            file_list += self.fetch_file_list(url, encoding, html_locator, info_locator, source_name, file_type, file_type_2)\n",
    "\n",
    "        return file_list\n",
    "    \n",
    "# print(Crawler().generate_file_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tr class=\"light\" valign=\"top\">\n",
      "<td align=\"right\" valign=\"top\" width=\"15\"><img oldid=\"613\" oldsrc=\"/_upload/tpl/00/1a/26/template26/res/ydot.gif\" related=\"1\" src=\"/_upload/tpl/00/1a/26/template26/res/ydot.gif\"/> </td>\n",
      "<td width=\"80%\"><a href=\"https://scc.ustc.edu.cn/zlsc/user_doc/html\" target=\"_blank\" title=\"用户使用手册[html版]\">用户使用手册[html版]</a></td>\n",
      "<td align=\"right\">2021-03-07</td></tr>, <tr class=\"light\" valign=\"top\">\n",
      "<td align=\"right\" valign=\"top\" width=\"15\"><img oldid=\"613\" oldsrc=\"/_upload/tpl/00/1a/26/template26/res/ydot.gif\" related=\"1\" src=\"/_upload/tpl/00/1a/26/template26/res/ydot.gif\"/> </td>\n",
      "<td width=\"80%\"><a href=\"https://scc.ustc.edu.cn/zlsc/faq/\" target=\"\" title=\"常见使用问题\">常见使用问题</a></td>\n",
      "<td align=\"right\">2018-10-01</td></tr>, <tr class=\"light\" valign=\"top\">\n",
      "<td align=\"right\" valign=\"top\" width=\"15\"><img oldid=\"613\" oldsrc=\"/_upload/tpl/00/1a/26/template26/res/ydot.gif\" related=\"1\" src=\"/_upload/tpl/00/1a/26/template26/res/ydot.gif\"/> </td>\n",
      "<td width=\"80%\"><a href=\"/2018/0926/c409a339006/page.htm\" target=\"_blank\" title=\"基于Google Authenticator二阶段密码验证SSH登录用户端用法\">基于Google Authenticator二阶段密码验证SSH登录用户端用法</a></td>\n",
      "<td align=\"right\">2018-01-07</td></tr>]\n",
      "[{'title': '用户使用手册[html版]', 'url': 'https://scc.ustc.edu.cn/zlsc/user_doc/html', 'time': '2021-03-07-用户使用手册[html版]', 'source': '超算中心', 'file_type': '常见使用问题', 'file_type_2': None}, {'title': '常见使用问题', 'url': 'https://scc.ustc.edu.cn/zlsc/faq/', 'time': '2018-10-01-常见使用问题', 'source': '超算中心', 'file_type': '常见使用问题', 'file_type_2': None}, {'title': '基于Google Authenticator二阶段密码验证SSH登录用户端用法', 'url': 'https://scc.ustc.edu.cn/2018/0926/c409a339006/page.htm', 'time': '2018-01-07-基于Google Authenticator二阶段密码验证SSH登录用户端用法', 'source': '超算中心', 'file_type': '常见使用问题', 'file_type_2': None}]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class Crawler_test:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def fetch_data(self, url):\n",
    "        \"\"\"Fetch data from url. Return the content of the response.\"\"\"\n",
    "        time.sleep(Config.SLEEP_TIME)  # sleep for n second to avoid being blocked\n",
    "        headers = {\"User-Agent\": Config.USER_AGENT, 'Cookie': Config.COOKIE}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.encoding = 'utf-8'\n",
    "        return response.text\n",
    "    \n",
    "    def fetch_file(self, url):\n",
    "        \"\"\"Fetch file from url. Return a list of bs4.element.Tag.\"\"\"\n",
    "        bs = BeautifulSoup(self.fetch_data(url), 'html.parser')\n",
    "\n",
    "        bs = bs.find('table', frag = '窗口1')\n",
    "        bs = bs.find_all('tr', class_ = 'light')\n",
    "\n",
    "        return bs\n",
    "\n",
    "    def fetch_file_list(self, url, source_name, file_type = None, file_type_2 = None):\n",
    "        \"\"\"Fetch file list from url. Return a list of dict.\"\"\"\n",
    "        bs_result = self.fetch_file(url)\n",
    "        \n",
    "        print(bs_result)\n",
    "\n",
    "        result = []\n",
    "\n",
    "        for i in range(len(bs_result)):\n",
    "            result.append(dict())\n",
    "            \n",
    "            result[i]['title'] = bs_result[i].find('td', width='80%').text.strip()\n",
    "            result[i]['url'] = urljoin(url, bs_result[i].find('a')['href'])\n",
    "            result[i]['time'] = bs_result[i].text.strip()\n",
    "            if result[i]['time'].count('\\n') != None and result[i]['time'].count('\\n') == 1:\n",
    "                parts = result[i]['time'].split('\\n')\n",
    "                result[i]['time'] = f'{parts[1]}-{parts[0]}'\n",
    "\n",
    "            result[i]['source'] = source_name\n",
    "            result[i]['file_type'] = file_type\n",
    "            result[i]['file_type_2'] = file_type_2\n",
    "\n",
    "        return result\n",
    "    \n",
    "print(Crawler_test().fetch_file_list('https://scc.ustc.edu.cn/409/list1.psp', '超算中心', '常见使用问题'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 91/91 [09:14<00:00,  6.09s/item]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>time</th>\n",
       "      <th>source</th>\n",
       "      <th>file_type</th>\n",
       "      <th>file_type_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>大数据学院学生离校请销假管理规定</td>\n",
       "      <td>http://sds.ustc.edu.cn/2021/1111/c15443a532371...</td>\n",
       "      <td>2022-11-28</td>\n",
       "      <td>大数据学院</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>公务用车平台设置使用手册</td>\n",
       "      <td>http://sds.ustc.edu.cn/2022/0628/c15443a561056...</td>\n",
       "      <td>2022-06-28</td>\n",
       "      <td>大数据学院</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>主题团日活动项目审批表</td>\n",
       "      <td>http://sds.ustc.edu.cn/2022/0519/c15443a555203...</td>\n",
       "      <td>2022-05-19</td>\n",
       "      <td>大数据学院</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>主题党日活动项目审批表</td>\n",
       "      <td>http://sds.ustc.edu.cn/2022/0519/c15443a555202...</td>\n",
       "      <td>2022-05-19</td>\n",
       "      <td>大数据学院</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>大数据学院网站信息发布审核制度</td>\n",
       "      <td>http://sds.ustc.edu.cn/2021/1122/c15443a535477...</td>\n",
       "      <td>2021-11-22</td>\n",
       "      <td>大数据学院</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959</th>\n",
       "      <td>新支部成立“三上三下”（各党总支所属党支部为例）</td>\n",
       "      <td>https://ses.ustc.edu.cn/2022/1009/c28343a57461...</td>\n",
       "      <td>2022-10-09</td>\n",
       "      <td>工程科学学院</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>支部换届“三上三下”（样例）</td>\n",
       "      <td>https://ses.ustc.edu.cn/2022/1009/c28343a57460...</td>\n",
       "      <td>2022-10-09</td>\n",
       "      <td>工程科学学院</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961</th>\n",
       "      <td>出国（境）党员保留、恢复组织关系审批表</td>\n",
       "      <td>https://ses.ustc.edu.cn/2022/1009/c28343a57460...</td>\n",
       "      <td>2022-10-09</td>\n",
       "      <td>工程科学学院</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962</th>\n",
       "      <td>支部日常工作常用材料模板</td>\n",
       "      <td>https://ses.ustc.edu.cn/2022/1009/c28343a57460...</td>\n",
       "      <td>2022-10-09</td>\n",
       "      <td>工程科学学院</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>发展党员材料模板</td>\n",
       "      <td>https://ses.ustc.edu.cn/2022/1009/c28343a57460...</td>\n",
       "      <td>2022-10-09</td>\n",
       "      <td>工程科学学院</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1964 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         title  \\\n",
       "0             大数据学院学生离校请销假管理规定   \n",
       "1                 公务用车平台设置使用手册   \n",
       "2                  主题团日活动项目审批表   \n",
       "3                  主题党日活动项目审批表   \n",
       "4              大数据学院网站信息发布审核制度   \n",
       "...                        ...   \n",
       "1959  新支部成立“三上三下”（各党总支所属党支部为例）   \n",
       "1960            支部换届“三上三下”（样例）   \n",
       "1961       出国（境）党员保留、恢复组织关系审批表   \n",
       "1962              支部日常工作常用材料模板   \n",
       "1963                  发展党员材料模板   \n",
       "\n",
       "                                                    url        time  source  \\\n",
       "0     http://sds.ustc.edu.cn/2021/1111/c15443a532371...  2022-11-28   大数据学院   \n",
       "1     http://sds.ustc.edu.cn/2022/0628/c15443a561056...  2022-06-28   大数据学院   \n",
       "2     http://sds.ustc.edu.cn/2022/0519/c15443a555203...  2022-05-19   大数据学院   \n",
       "3     http://sds.ustc.edu.cn/2022/0519/c15443a555202...  2022-05-19   大数据学院   \n",
       "4     http://sds.ustc.edu.cn/2021/1122/c15443a535477...  2021-11-22   大数据学院   \n",
       "...                                                 ...         ...     ...   \n",
       "1959  https://ses.ustc.edu.cn/2022/1009/c28343a57461...  2022-10-09  工程科学学院   \n",
       "1960  https://ses.ustc.edu.cn/2022/1009/c28343a57460...  2022-10-09  工程科学学院   \n",
       "1961  https://ses.ustc.edu.cn/2022/1009/c28343a57460...  2022-10-09  工程科学学院   \n",
       "1962  https://ses.ustc.edu.cn/2022/1009/c28343a57460...  2022-10-09  工程科学学院   \n",
       "1963  https://ses.ustc.edu.cn/2022/1009/c28343a57460...  2022-10-09  工程科学学院   \n",
       "\n",
       "     file_type file_type_2  \n",
       "0         None        None  \n",
       "1         None        None  \n",
       "2         None        None  \n",
       "3         None        None  \n",
       "4         None        None  \n",
       "...        ...         ...  \n",
       "1959      None        None  \n",
       "1960      None        None  \n",
       "1961      None        None  \n",
       "1962      None        None  \n",
       "1963      None        None  \n",
       "\n",
       "[1964 rows x 6 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 翻页功能部分函数, 在教务处网站上已经基本实现翻页功能\n",
    "\n",
    "class Crawler_2:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def fetch_data(self, url, encoding):\n",
    "        \"\"\"Fetch data from url. Return the content of the response.\"\"\"\n",
    "        time.sleep(Config.SLEEP_TIME)  # sleep for n second to avoid being blocked\n",
    "        headers = {\"User-Agent\": Config.USER_AGENT, 'Cookie': Config.COOKIE}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if encoding:\n",
    "            response.encoding = encoding\n",
    "        return response.text\n",
    "    \n",
    "    def fetch_file(self, url, html_locator, encoding):\n",
    "        \"\"\"Fetch file from url. Return a list of bs4.element.Tag.\"\"\"\n",
    "        bs = BeautifulSoup(self.fetch_data(url, encoding), 'html.parser')\n",
    "\n",
    "        for operation in html_locator:\n",
    "            method = operation['method']\n",
    "            args = operation['args']\n",
    "            kwargs = operation.get('kwargs', None)\n",
    "\n",
    "            if method == 'find':\n",
    "                if kwargs:\n",
    "                    bs = bs.find(args, **kwargs)\n",
    "                else:\n",
    "                    bs = bs.find(args)\n",
    "\n",
    "            elif method == 'find_all':\n",
    "                if kwargs:\n",
    "                    bs = bs.find_all(args, **kwargs)\n",
    "                else:\n",
    "                    bs = bs.find_all(args)\n",
    "            \n",
    "            if bs == None:\n",
    "                return None\n",
    "\n",
    "        return bs\n",
    "    \n",
    "    def get_info(self, bs_result, url, method, args, kargs, args2):\n",
    "\n",
    "        if method == 'find' and kargs == None:\n",
    "            if args2 == 'text':\n",
    "                return bs_result.find(args).text.strip()\n",
    "            elif args2 == 'href':\n",
    "                return urljoin(url, bs_result.find(args)[args2])\n",
    "            else:\n",
    "                return bs_result.find(args)[args2]\n",
    "\n",
    "        elif method == 'find' and kargs != None:\n",
    "            if args2 == 'text':\n",
    "                return bs_result.find(args, **kargs).text.strip()\n",
    "            elif args2 == 'href':\n",
    "                return urljoin(url, bs_result.find(args, **kargs)[args2])\n",
    "            else:\n",
    "                return bs_result.find(args, **kargs)[args2]\n",
    "\n",
    "        # elif method == None:\n",
    "        #     return None\n",
    "        elif method == None:\n",
    "            if args2 == 'text':\n",
    "                return bs_result.text.strip()\n",
    "            elif args2 == 'href':\n",
    "                return urljoin(url, bs_result[args2])\n",
    "\n",
    "    def fetch_file_list(self, url, encoding, html_locator, info_locator , source_name, file_type = None, file_type_2 = None):\n",
    "        \"\"\"Fetch file list from url. Return a list of dict.\"\"\"\n",
    "\n",
    "        # 极其特殊的先研院网站翻页机制\n",
    "        if url == 'https://iat.ustc.edu.cn/iat/x161/index_1.html':\n",
    "            url = 'https://iat.ustc.edu.cn/iat/x161/index.html'\n",
    "\n",
    "        bs_result = self.fetch_file(url, html_locator, encoding)\n",
    "\n",
    "        if bs_result == None:\n",
    "            return None\n",
    "        \n",
    "        result = []\n",
    "\n",
    "        for i in range(len(bs_result)):\n",
    "            result.append(dict())\n",
    "            \n",
    "            for operation in info_locator:\n",
    "                info = operation['info']\n",
    "                method = operation['method']\n",
    "                args = operation['args']\n",
    "                args2 = operation['args2']\n",
    "                kargs = None\n",
    "\n",
    "                if type(args) == list:\n",
    "                    kargs = args[1]\n",
    "                    args = args[0]\n",
    "\n",
    "                result[i][info] = self.get_info(bs_result[i], url, method, args, kargs, args2)\n",
    "\n",
    "            \n",
    "            if result[i]['time'] == None or result[i]['time'] == '': \n",
    "                pass\n",
    "\n",
    "            elif result[i]['time'].count('\\n') == 1:\n",
    "                parts = result[i]['time'].split('\\n')\n",
    "                result[i]['time'] = f'{parts[1]}-{parts[0]}'\n",
    "\n",
    "            elif '发布时间' in result[i]['time']:\n",
    "                result[i]['time'] = result[i]['time'][5:15]\n",
    "\n",
    "            elif '[' == result[i]['time'][0]:\n",
    "                result[i]['time'] = result[i]['time'][1:-1]\n",
    "\n",
    "            elif result[i]['time'].count('年') == 1 and result[i]['time'].count('月') == 1 and result[i]['time'].count('日') == 1:\n",
    "                result[i]['time'] = result[i]['time'].replace('年', '-').replace('月', '-').replace('日', '')\n",
    "\n",
    "            # 检测时间末尾是否带有标题，如有，则去除时间中的标题。超算中心网站特性\n",
    "            if result[i]['title'] in result[i]['time']:\n",
    "                result[i]['time'] = result[i]['time'].replace(result[i]['title'], '').strip()[:-1]\n",
    "            \n",
    "            # 检测标题末尾是否有[xxxx-xx-xx]的时间信息，如有，在标题中去除，并将对应内容移动到时间处。时间处内容不包括括号(软件学院网站bug)\n",
    "            if result[i]['title'][-11:-1].count('-') == 2:\n",
    "                result[i]['time'] = result[i]['title'][-11:-1]\n",
    "                result[i]['title'] = result[i]['title'][:-13]\n",
    "            \n",
    "\n",
    "            result[i]['source'] = source_name\n",
    "            result[i]['file_type'] = file_type\n",
    "            result[i]['file_type_2'] = file_type_2\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def generate_file_list(self):\n",
    "        file_list = []\n",
    "        with open('data.json', 'r') as json_file:\n",
    "            data_list = json.load(json_file)\n",
    "\n",
    "        for data in tqdm(data_list, desc='Processing', unit='item'):\n",
    "            url = data['url']\n",
    "            encoding = data['encoding']\n",
    "            html_locator = data['html_locator']\n",
    "            info_locator = data['info_locator']\n",
    "            source_name = data['title']\n",
    "            file_type = data['dtype']\n",
    "            file_type_2 = data['dtype2']\n",
    "            flip = data['flip']\n",
    "            \n",
    "            if flip == False:\n",
    "                file_list += self.fetch_file_list(url, encoding, html_locator, info_locator, source_name, file_type, file_type_2)\n",
    "\n",
    "            else:\n",
    "                for i in range(1, 100):\n",
    "                    flip_url = url.replace('{page_num}', str(i))\n",
    "                    flip_result = self.fetch_file_list(flip_url, encoding, html_locator, info_locator, source_name, file_type, file_type_2)\n",
    "                    if flip_result == None or len(flip_result) == 0:\n",
    "                        break\n",
    "                    else:\n",
    "                        file_list += flip_result\n",
    "\n",
    "\n",
    "        return file_list\n",
    "    \n",
    "    def generate_file_csv(self):\n",
    "        file_list = self.generate_file_list()\n",
    "        df = pd.DataFrame(file_list)\n",
    "        df.to_csv('file_list.csv', index=False)\n",
    "        return df\n",
    "\n",
    "Crawler_2().generate_file_csv()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
